var documenterSearchIndex = {"docs":
[{"location":"tutorials/#Surrogates-101-1","page":"Basics","title":"Surrogates 101","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's start with something easy to get our hands dirty. I want to build a surrogate for f(x) = log(x)*x^2+x^3. Let's choose the radial basis surrogate.","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nf = x -> log(x)*x^2+x^3\nlb = 1.0\nub = 10.0\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub,rad=thinplateRadial)\n\n#I want an approximation at 5.4\napprox = my_radial_basis(5.4)","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's now see an example in 2D.","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nusing LinearAlgebra\nf = x -> x[1]*x[2]\nlb = [1.0,2.0]\nub = [10.0,8.5]\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub)\n\n#I want an approximation at (1.0,1.4)\napprox = my_radial_basis((1.0,1.4))","category":"page"},{"location":"tutorials/#Kriging-standard-error-1","page":"Basics","title":"Kriging standard error","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's now use the Kriging surrogate, which is a single-output Gaussian process. This surrogate has a nice feature: not only does it approximate the solution at a point, it also calculates the standard error at such point. Let's see an example:","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nf = x -> exp(x)*x^2+x^3\nlb = 0.0\nub = 10.0\nx = sample(100,lb,ub,UniformSample())\ny = f.(x)\np = 1.9\nmy_krig = Kriging(x,y,lb,ub,p=p)\n\n#I want an approximation at 5.4\napprox = my_krig(5.4)\n\n#I want to find the standard error at 5.4\nstd_err = std_error_at_point(my_krig,5.4)","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Let's now optimize the Kriging surrogate using Lower confidence bound method, this is just a one-liner:","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"surrogate_optimize(f,LCBS(),lb,ub,my_krig,UniformSample())","category":"page"},{"location":"tutorials/#Lobachesky-integral-1","page":"Basics","title":"Lobachesky integral","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"The Lobachesky surrogate has the nice feature of having a closed formula for its integral, which is something that other surrogates are missing. Let's compare it with QuadGK:","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nusing QuadGK\nobj = x -> 3*x + log(x)\na = 1.0\nb = 4.0\nx = sample(2000,a,b,SobolSample())\ny = obj.(x)\nalpha = 2.0\nn = 6\nmy_loba = LobacheskySurrogate(x,y,a,b,alpha=alpha,n=n)\n\n#1D integral\nint_1D = lobachesky_integral(my_loba,a,b)\nint = quadgk(obj,a,b)\nint_val_true = int[1]-int[2]\n@assert int_1D â‰ˆ int_val_true","category":"page"},{"location":"tutorials/#Example-of-NeuralSurrogate-1","page":"Basics","title":"Example of NeuralSurrogate","text":"","category":"section"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"Basic example of fitting a neural network on a simple function of two variables.","category":"page"},{"location":"tutorials/#","page":"Basics","title":"Basics","text":"using Surrogates\nusing Flux\nusing Statistics\n\nf = x -> x[1]^2 + x[2]^2\nbounds = Float32[-1.0, -1.0], Float32[1.0, 1.0]\n# Flux models are in single precision by default.\n# Thus, single precision will also be used here for our training samples.\n\nx_train = sample(100, bounds..., SobolSample())\ny_train = f.(x_train)\n\n# Perceptron with one hidden layer of 20 neurons.\nmodel = Chain(Dense(2, 20, relu), Dense(20, 1))\nloss(x, y) = Flux.mse(model(x), y)\n\n# Training of the neural network\nlearning_rate = 0.1\noptimizer = Descent(learning_rate)  # Simple gradient descent. See Flux documentation for other options.\nn_epochs = 50\nsgt = NeuralSurrogate(x_train, y_train, bounds..., model=model, loss=loss, opt=optimizer, n_echos=n_epochs)\n\n# Testing the new model\nx_test = sample(30, bounds..., SobolSample())\ntest_error = mean(abs2, sgt(x)[1] - f(x) for x in x_test)","category":"page"},{"location":"radials/#Radial-Surrogates-1","page":"Radials","title":"Radial Surrogates","text":"","category":"section"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Let's start with something easy to get our hands dirty. I want to build a surrogate for f(x) = log(x)*x^2+x^3. Let's choose the Radial Basis Surrogate for 1D.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"using Surrogates\r\nf = x -> log(x)*x^2+x^3\r\nlb = 1.0\r\nub = 10.0\r\nx = sample(50,lb,ub,SobolSample())\r\ny = f.(x)\r\nmy_radial_basis = RadialBasis(x,y,lb,ub)\r\n\r\n#I want an approximation at 5.4\r\napprox = my_radial_basis(5.4)","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"For each Surrogates we can call it with different inputs: either (xylbub) or with it's parameters,","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"different for each Surrogates. Let's see for Radial Basis Surrogates:","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"my_radial_basis = RadialBasis(x,y,lb,ub,rad=thinplateRadial)\r\n\r\n#We want an approximation at 5.4\r\napprox = my_radial_basis(5.4)","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Now, Let's choose the Radial Basis Surrogate for 2D.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"using Surrogates\r\nusing LinearAlgebra\r\nf = x -> x[1]*x[2]\r\nlb = [1.0,2.0]\r\nub = [10.0,8.5]\r\nx = sample(50,lb,ub,SobolSample())\r\ny = f.(x)\r\nmy_radial_basis = RadialBasis(x,y,lb,ub)\r\n\r\n#I want an approximation at (1.0,1.4)\r\napprox = my_radial_basis((1.0,1.4))","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Now we will call an Optimization Method for RadialBasis Surrogates in 1D and ND.","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"Let's see an Optimization method for 1D:","category":"page"},{"location":"radials/#","page":"Radials","title":"Radials","text":"using Surrogates, LinearAlgebra\r\n##### For 1D #####\r\nlb = 0.0\r\nub = 15.0\r\nobjective_function = x -> 2*x+1\r\nx = [2.5,4.0,6.0]\r\ny = [6.0,9.0,13.0]\r\na = 0.0\r\nb = 6.0\r\n\r\nmy_rad_SRBF1 = RadialBasis(x,y,a,b,rad = linearRadial)\r\nsurrogate_optimize(objective_function,SRBF(),a,b,my_rad_SRBF1,UniformSample())","category":"page"},{"location":"contributing/#Contributions-1","page":"Contributing","title":"Contributions","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Contributions are very welcome! There are many ways do help:","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Opening/solving issues\nMaking the code more efficient\nOpening a new PR with a new Sampling technique, Surrogate or optimization method\nWriting more tutorials with your own unique use case of the library\nYour own idea!","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"You can also contact me on the Julia slack channel at @ludoro.","category":"page"},{"location":"contributing/#List-of-contributors-1","page":"Contributing","title":"List of contributors","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Ludovico Bessi (@ludoro)\nChris Rackauckas\nRohit Singh Rathaur (@TeAmp0is0N)","category":"page"},{"location":"kriging/#Kriging-surrogate-tutorial-1","page":"Kriging","title":"Kriging surrogate tutorial","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Kriging or Gaussian process regression is a method of interpolation for which the interpolated values are modeled by a Gaussian process.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"We are going to use a Kriging surrogate to optimize f(x)=(6x-2)^2sin(12x-4). (function from Forrester et al. (2008)).","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"using Surrogates\nusing Plots","category":"page"},{"location":"kriging/#Sampling-1","page":"Kriging","title":"Sampling","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"We choose to sample f in 4 points between 0 and 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"# https://www.sfu.ca/~ssurjano/forretal08.html\n# Forrester et al. (2008) Function\nf(x) = (6 * x - 2)^2 * sin(12 * x - 4)\n\nn_samples = 4\nlower_bound = 0.0\nupper_bound = 1.0\n\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\n\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\")","category":"page"},{"location":"kriging/#Building-a-surrogate-1","page":"Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"With our sampled points we can build the Kriging surrogate using the Kriging function.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"kriging_surrogate behaves like an ordinary function which we can simply plot. A nice statistical property of this surrogate is being able to calculate the error of the function at each point, we plot this as a confidence interval using the ribbon argument.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"kriging_surrogate = Kriging(x, y, lower_bound, upper_bound, p=1.9);\n\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\")\nplot!(kriging_surrogate, label=\"Surrogate function\", ribbon=p->std_error_at_point(kriging_surrogate, p))","category":"page"},{"location":"kriging/#Optimizing-1","page":"Kriging","title":"Optimizing","text":"","category":"section"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"Having built a surrogate, we can now use it to search for minimas in our original function f.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"kriging/#","page":"Kriging","title":"Kriging","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, kriging_surrogate, SobolSample())\n\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\")\nplot!(kriging_surrogate, label=\"Surrogate function\", ribbon=p->std_error_at_point(kriging_surrogate, p))","category":"page"},{"location":"surrogate/#Surrogate-1","page":"Surrogates","title":"Surrogate","text":"","category":"section"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Every surrogate has a different definition depending on the parameters needed. However, they have in common:","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"add_point!(::AbstractSurrogate,x_new,y_new)\nAbstractSurrogate(value)","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"The first function adds a sample point to the surrogate, thus changing the internal coefficients. The second one calculates the approximation at value.","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Linear surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"LinearSurrogate(x,y,lb,ub)","category":"page"},{"location":"surrogate/#Surrogates.LinearSurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.LinearSurrogate","text":"LinearSurrogate(x,y,lb,ub)\n\nBuilds a linear surrogate using GLM.jl\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Radial basis function surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"RadialBasis(x, y, lb, ub; rad::RadialFunction = linearRadial, scale_factor::Real=1.0, sparse = false)","category":"page"},{"location":"surrogate/#Surrogates.RadialBasis-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.RadialBasis","text":"RadialBasis(x,y,lb,ub,rad::RadialFunction, scale_factor::Float = 1.0)\n\nConstructor for RadialBasis surrogate\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Kriging surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Kriging(x,y,p,theta)","category":"page"},{"location":"surrogate/#Surrogates.Kriging-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.Kriging","text":"Kriging(x,y,lb,ub;p=collect(one.(x[1])),theta=collect(one.(x[1])))\n\nConstructor for Kriging surrogate.\n\n(x,y): sampled points\np: array of values 0<=p<2 modeling the    smoothness of the function being approximated in the i-th variable.    low p -> rough, high p -> smooth\ntheta: array of values > 0 modeling how much the function is         changing in the i-th variable.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Lobachesky surrogate","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"LobacheskySurrogate(x,y,lb,ub; alpha = collect(one.(x[1])),n::Int = 4, sparse = false)\nlobachesky_integral(loba::LobacheskySurrogate,lb,ub)","category":"page"},{"location":"surrogate/#Surrogates.LobacheskySurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.LobacheskySurrogate","text":"LobacheskySurrogate(x,y,alpha,n::Int,lb,ub,sparse = false)\n\nBuild the Lobachesky surrogate with parameters alpha and n.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#Surrogates.lobachesky_integral-Tuple{LobacheskySurrogate,Any,Any}","page":"Surrogates","title":"Surrogates.lobachesky_integral","text":"lobachesky_integral(loba::LobacheskySurrogate,lb,ub)\n\nCalculates the integral of the Lobachesky surrogate, which has a closed form.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Support vector machine surrogate, requires using LIBSVM","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"SVMSurrogate(x,y,lb::Number,ub::Number)","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Random forest surrogate, requires using XGBoost","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"RandomForestSurrogate(x,y,lb,ub;num_round::Int = 1)","category":"page"},{"location":"surrogate/#Surrogates.RandomForestSurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.RandomForestSurrogate","text":"RandomForestSurrogate(x,y,lb,ub,num_round)\n\nBuild Random forest surrogate. num_round is the number of trees.\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Neural network surrogate, requires using Flux","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"NeuralSurrogate(x,y,lb,ub; model = Chain(Dense(length(x[1]),1), first), loss = (x,y) -> Flux.mse(model(x), y),opt = Descent(0.01),n_echos::Int = 1)","category":"page"},{"location":"surrogate/#Surrogates.NeuralSurrogate-NTuple{4,Any}","page":"Surrogates","title":"Surrogates.NeuralSurrogate","text":"NeuralSurrogate(x,y,lb,ub,model,loss,opt,n_echos)\n\nmodel: Flux layers\nloss: loss function\nopt: optimization function\n\n\n\n\n\n","category":"method"},{"location":"surrogate/#Creating-another-surrogate-1","page":"Surrogates","title":"Creating another surrogate","text":"","category":"section"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"It's great that you want to add another surrogate to the library! You will need to:","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Define a new mutable struct and a constructor function\nDefine add_point!(your_surrogate::AbstactSurrogate,x_new,y_new)\nDefine your_surrogate(value) for the approximation","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"Example","category":"page"},{"location":"surrogate/#","page":"Surrogates","title":"Surrogates","text":"mutable struct NewSurrogate{X,Y,L,U,C,A,B} <: AbstractSurrogate\n  x::X\n  y::Y\n  lb::L\n  ub::U\n  coeff::C\n  alpha::A\n  beta::B\nend\n\nfunction NewSurrogate(x,y,lb,ub,parameters)\n    ...\n    return NewSurrogate(x,y,lb,ub,calculated\\_coeff,alpha,beta)\nend\n\nfunction add_point!(NewSurrogate,x\\_new,y\\_new)\n\n  nothing\nend\n\nfunction (s::NewSurrogate)(value)\n  return s.coeff*value + s.alpha\nend","category":"page"},{"location":"samples/#Samples-1","page":"Samples","title":"Samples","text":"","category":"section"},{"location":"samples/#","page":"Samples","title":"Samples","text":"The syntax for sampling in an interval or region is the following:","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,S::SamplingAlgorithm)","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"where lb and ub are, respectively, the lower and upper bounds. There are many sampling algorithms to choose from:","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Grid sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"GridSample{T}\nsample(n,lb,ub,S::GridSample)","category":"page"},{"location":"samples/#Surrogates.GridSample","page":"Samples","title":"Surrogates.GridSample","text":"GridSample{T}\n\nT is the step dx for lb:dx:ub\n\n\n\n\n\n","category":"type"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,GridSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,S::GridSample)\n\nReturns a tuple containing numbers in a grid.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Uniform sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::UniformSample)","category":"page"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,UniformSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,::UniformRandom)\n\nReturns a Tuple containing uniform random numbers.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Sobol sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::SobolSample)","category":"page"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,SobolSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,::SobolSampling)\n\nReturns a Tuple containing Sobol sequences.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Latin Hypercube sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::LatinHypercubeSample)","category":"page"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,LatinHypercubeSample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,::LatinHypercube)\n\nReturns a Tuple containing LatinHypercube sequences.\n\n\n\n\n\n","category":"method"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Low Discrepancy sample","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"LowDiscrepancySample{T}\nsample(n,lb,ub,S::LowDiscrepancySample)","category":"page"},{"location":"samples/#Surrogates.LowDiscrepancySample","page":"Samples","title":"Surrogates.LowDiscrepancySample","text":"LowDiscrepancySample{T}\n\nT is the base for the sequence\n\n\n\n\n\n","category":"type"},{"location":"samples/#Surrogates.sample-Tuple{Any,Any,Any,LowDiscrepancySample}","page":"Samples","title":"Surrogates.sample","text":"sample(n,lb,ub,S::LowDiscrepancySample)\n\nLow discrepancy sample:\n\nDimension 1: Van der Corput sequence\nDimension > 1: Halton sequence\n\nIf dimension d > 1, all bases must be coprime with each other.\n\n\n\n\n\n","category":"method"},{"location":"samples/#Adding-a-new-sampling-method-1","page":"Samples","title":"Adding a new sampling method","text":"","category":"section"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Adding a new sampling method is a two- step process:","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Adding a new SamplingAlgorithm type\nOverloading the sample function with the new type.","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"Example","category":"page"},{"location":"samples/#","page":"Samples","title":"Samples","text":"struct NewAmazingSamplingAlgorithm{OPTIONAL} <: SamplingAlgorithm end\n\nfunction sample(n,lb,ub,::NewAmazingSamplingAlgorithm)\n    if lb is  Number\n        ...\n        return x\n    else\n        ...\n        return Tuple.(x)\n    end\nend","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"(Image: SurrogatesLogo)","category":"page"},{"location":"#Overview-1","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"A surrogate model is an approximation method that mimics the behavior of a computationally expensive simulation. In more mathematical terms: suppose we are attempting to optimize a function  f(p), but each calculation of  f is very expensive. It may be the case that we need to solve a PDE for each point or use advanced numerical linear algebra machinery, which is usually costly. The idea is then to develop a surrogate model  g which approximates  f by training on previous data collected from evaluations of  f. The construction of a surrogate model can be seen as a three-step process:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Sample selection\nConstruction of the surrogate model\nSurrogate optimization","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The sampling methods are super important for the behavior of the Surrogate. At the moment they are:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Grid sample\nUniform sample\nSobol sample\nLatin Hypercube sample\nLow discrepancy sample","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"The available surrogates are:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Linear\nRadial Basis\nKriging\nCustom Kriging provided with Stheno\nNeural Network\nSupport Vector Machine\nRandom Forest\nSecond Order Polynomial\nInverse Distance","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"After the surrogate is built, we need to optimize it with respect to some objective function. That is, simultaneously looking for a minimum and sampling the most unknown region. The available optimization methods are:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Stochastic RBF (SRBF)\nLower confidence bound strategy (LCBS)\nExpected improvement (EI)\nDynamic coordinate search (DYCORS)","category":"page"},{"location":"#Multi-output-Surrogates-1","page":"Overview","title":"Multi-output Surrogates","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"In certain situations, the function being modeled may have a multi-dimensional output space. In such a case, the surrogate models can take advantage of correlations between the observed output variables to obtain more accurate predictions.","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"When constructing the original surrogate, each element of the passed y vector should itself be a vector. For example, the following y are all valid.","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"using Surrogates\nusing StaticArrays\n\nx = sample(5, [0.0; 0.0], [1.0; 1.0], SobolSample())\nf_static = (x) -> StaticVector(x[1], log(x[2]*x[1]))\nf = (x) -> [x, log(x)/2]\n\ny = f_static.(x)\ny = f.(x)","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Currently, the following are implemented as multi-output surrogates:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Radial Basis\nNeural Network (via Flux)\nSecond Order Polynomial\nInverse Distance\nCustom Kriging (via Stheno)","category":"page"},{"location":"#Gradients-1","page":"Overview","title":"Gradients","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"The surrogates implemented here are all automatically differentiable via Zygote. Because of this property, surrogates are useful models for processes which aren't explicitly differentiable, and can be used as layers in, for instance, Flux models.","category":"page"},{"location":"#Installation-1","page":"Overview","title":"Installation","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"Surrogates is registered in the Julia General Registry. In the REPL:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"]add Surrogates","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"You can obtain the current master with:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"]add https://github.com/JuliaDiffEq/Surrogates.jl#master","category":"page"},{"location":"#Quick-example-1","page":"Overview","title":"Quick example","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"using Surrogates\nnum_samples = 10\nlb = 0.0\nub = 10.0\n\n#Sampling\nx = sample(num_samples,lb,ub,SobolSample())\nf = x-> log(x)*x^2+x^3\ny = f.(x)\n\n#Creating surrogate\nalpha = 2.0\nn = 6\nmy_lobachesky = LobacheskySurrogate(x,y,lb,ub,alpha=alpha,n=n)\n\n#Approximating value at 5.0\nvalue = my_lobachesky(5.0)\n\n#Adding more data points\nsurrogate_optimize(f,SRBF(),lb,ub,my_lobachesky,UniformSample())\n\n#New approximation\nvalue = my_lobachesky(5.0)","category":"page"},{"location":"optimizations/#Optimization-techniques-1","page":"Optimization","title":"Optimization techniques","text":"","category":"section"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"SRBF","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::SRBF,lb,ub,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,SRBF,Any,Any,AbstractSurrogate,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"The main idea is to pick the new evaluations from a set of candidate points where each candidate point is generated as an N(0, sigma^2) distributed perturbation from the current best solution. The value of sigma is modified based on progress and follows the same logic as in many trust region methods: we increase sigma if we make a lot of progress (the surrogate is accurate) and decrease sigma when we arenâ€™t able to make progress (the surrogate model is inaccurate). More details about how sigma is updated is given in the original papers.\n\nAfter generating the candidate points, we predict their objective function value and compute the minimum distance to the previously evaluated point. Let the candidate points be denoted by C and let the function value predictions be s(x_i) and the distance values be d(x_i), both rescaled through a linear transformation to the interval [0,1]. This is done to put the values on the same scale. The next point selected for evaluation is the candidate point x that minimizes the weighted-distance merit function:\n\nmerit(x) = ws(x) + (1-w)(1-d(x))\n\nwhere 0 leq w leq 1. That is, we want a small function value prediction and a large minimum distance from the previously evaluated points. The weight w is commonly cycled between a few values to achieve both exploitation and exploration. When w is close to zero, we do pure exploration, while w close to 1 corresponds to exploitation.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"LCBS","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::LCBS,lb,ub,krig,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,LCBS,Any,Any,Any,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"This is an implementation of Lower Confidence Bound (LCB), a popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to minimize:\n\nLCB(x) = Ex - k * sqrt(Vx)\n\ndefault value k = 2.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"EI","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::EI,lb,ub,krig,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,EI,Any,Any,Any,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"This is an implementation of Expected Improvement (EI), arguably the most popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to maximize expected improvement:\n\nEI(x) = Emax(f_best-f(x)0)\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"DYCORS","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::DYCORS,lb,ub,surrn::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"optimizations/#Surrogates.surrogate_optimize-Tuple{Function,DYCORS,Any,Any,AbstractSurrogate,SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"  surrogate_optimize(obj::Function,::DYCORS,lb::Number,ub::Number,surr1::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)\n\nThis is an implementation of the DYCORS strategy by Regis and Shoemaker: Rommel G Regis and Christine A Shoemaker. Combining radial basis function surrogates and dynamic coordinate search in high-dimensional expensive black-box optimization. Engineering Optimization, 45(5): 529â€“555, 2013. This is an extension of the SRBF strategy that changes how the candidate points are generated. The main idea is that many objective functions depend only on a few directions so it may be advantageous to perturb only a few directions. In particular, we use a perturbation probability to perturb a given coordinate and decrease this probability after each function evaluation so fewer coordinates are perturbed later in the optimization.\n\n\n\n\n\n","category":"method"},{"location":"optimizations/#Adding-another-optimization-method-1","page":"Optimization","title":"Adding another optimization method","text":"","category":"section"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"To add another optimization method, you just need to define a new SurrogateOptimizationAlgorithm and write its corresponding algorithm, overloading the following:","category":"page"},{"location":"optimizations/#","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::NewOptimizatonType,lb,ub,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"}]
}
